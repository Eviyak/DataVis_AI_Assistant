import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import openai
import io
import json
import warnings
from sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from datetime import datetime
from pandas.api.types import is_numeric_dtype, is_categorical_dtype

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="DataJournalist ML Assistant",
    page_icon="üì∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ API –∫–ª—é—á–∞ OpenAI
if 'OPENAI_API_KEY' in st.secrets:
    openai.api_key = st.secrets['OPENAI_API_KEY']
else:
    openai.api_key = st.sidebar.text_input("–í–≤–µ–¥–∏—Ç–µ OpenAI API –∫–ª—é—á:", type="password")

# –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è
st.markdown("""
    <style>
        .stApp { background-color: #f0f2f6; color: #000000; }
        footer { visibility: hidden; }
        .stProgress > div > div > div > div { background: linear-gradient(to right, #ff4b4b, #ff9a9e); }
        .stButton>button { width: 100%; }
    </style>
""", unsafe_allow_html=True)

st.title("üì∞ DataJournalist ML Assistant")
st.markdown("""
    <div style="background-color:#ffffff;padding:20px;border-radius:10px;margin-bottom:20px;box-shadow:0 4px 6px rgba(0,0,0,0.1);">
    <h3 style="color:#333;margin-top:0;">üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ ML –¥–ª—è –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–æ–≤</h3>
    <p style="color:#666;">–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ ‚Üí –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–¥–∞—á—É ‚Üí –ü–æ–ª—É—á–∏—Ç–µ –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –∏–Ω—Å–∞–π—Ç—ã</p>
    </div>
""", unsafe_allow_html=True)

@st.cache_data(show_spinner="–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ... ‚è≥", ttl=3600, max_entries=3)
def load_data(uploaded_file):
    try:
        file_bytes = uploaded_file.read()
        if uploaded_file.name.endswith('.csv'):
            return pd.read_csv(io.BytesIO(file_bytes)), None
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            return pd.read_excel(io.BytesIO(file_bytes)), None
        elif uploaded_file.name.endswith('.json'):
            data = json.loads(file_bytes.decode('utf-8'))
            return pd.json_normalize(data), None
    except Exception as e:
        return None, f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}"

def reduce_mem_usage(df):
    start_mem = df.memory_usage().sum() / 1024**2
    for col in df.columns:
        col_type = df[col].dtype
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col].astype(np.int64)
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                if pd.api.types.is_numeric_dtype(df[col]):
                    try:
                        if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                            df[col] = df[col].astype(np.float16)
                        elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                            df[col] = df[col].astype(np.float32)
                        else:
                            df[col] = df[col].astype(np.float64)
                    except:
                        # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–∏–ø
                        df[col] = df[col].astype(np.float64)
    
    end_mem = df.memory_usage().sum() / 1024**2
    st.sidebar.info(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏: {start_mem:.2f} MB ‚Üí {end_mem:.2f} MB (—Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ {100*(start_mem-end_mem)/start_mem:.1f}%)")
    return df

def fill_missing_values(df):
    df_filled = df.copy()
    for col in df_filled.columns:
        if df_filled[col].isnull().sum() > 0:
            if is_numeric_dtype(df_filled[col]):
                df_filled[col].fillna(df_filled[col].median(), inplace=True)
            else:
                mode_val = df_filled[col].mode()
                if not mode_val.empty:
                    df_filled[col].fillna(mode_val[0], inplace=True)
                else:
                    df_filled[col].fillna("Unknown", inplace=True)
    return df_filled

def mark_anomalies(df):
    num_cols = df.select_dtypes(include=np.number).columns.tolist()
    
    if len(num_cols) == 0:
        return df
    
    df_processed = df.copy()
    
    X = df_processed[num_cols].values
    X = np.nan_to_num(X)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        iso = IsolationForest(contamination=0.05, random_state=42)
        preds = iso.fit_predict(X)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ DataFrame
        df_processed['anomaly'] = preds
        df_processed['anomaly'] = df_processed['anomaly'].map({1: 0, -1: 1})
        
        return df_processed
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {str(e)}")
        return df

def prepare_data_for_ml(df, target_column):
    le = LabelEncoder()
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = le.fit_transform(df[col].astype(str))
    
    X = df.drop(columns=[target_column])
    y = df[target_column]
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, scaler

def train_model(X, y, problem_type):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    cm = None
    
    if problem_type == "classification":
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model_name = "Random Forest (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)"
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        metrics = {"–¢–æ—á–Ω–æ—Å—Ç—å": accuracy}
    else:
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model_name = "Random Forest (–†–µ–≥—Ä–µ—Å—Å–∏—è)"
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        metrics = {"RMSE": rmse, "MSE": mse}
    
    return model, metrics, X_test, y_test, y_pred, cm

def generate_shap_plot(model, X, feature_names):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    
    if isinstance(shap_values, list):
        shap.summary_plot(shap_values[1], X, feature_names=feature_names, show=False)
    else:
        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
    
    plt.tight_layout()
    return plt.gcf()

def generate_ai_report(df, model, problem_type, target, metrics):
    if not openai.api_key:
        return "üîë –ö–ª—é—á OpenAI API –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö."
    
    prompt = f"""
–¢—ã - –∂—É—Ä–Ω–∞–ª–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –æ–ø—ã—Ç–æ–º –≤ data science. –ü–æ–¥–≥–æ—Ç–æ–≤—å –æ—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

–î–∞–Ω–Ω—ã–µ:
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {df.shape[0]}
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df.shape[1]}
- –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target}
- –¢–∏–ø –∑–∞–¥–∞—á–∏: {'–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è' if problem_type == 'classification' else '–†–µ–≥—Ä–µ—Å—Å–∏—è'}

–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏:
{json.dumps(metrics, indent=2)}

–í–∞–∂–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 5):
{df.columns.tolist()[:5]}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π:
1. –ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á—Ç–æ –¥–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å
2. –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –æ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
3. –ö–∞–∫ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ç–∞—Ç—å–µ
4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é

–ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ, –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∂–∞—Ä–≥–æ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏.
"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "–¢—ã –∂—É—Ä–Ω–∞–ª–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Å–ª–æ–∂–Ω—ã–µ ML-–∫–æ–Ω—Ü–µ–ø—Ç—ã –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.6,
            max_tokens=1500
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI API: {str(e)}"

def generate_flourish_recommendations(df, target):
    if not openai.api_key:
        return None
    
    prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: {list(df.columns)} –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π '{target}', 
–ø—Ä–µ–¥–ª–æ–∂–∏ 3 –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è Flourish. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∫–∞–∂–∏:

1. –¢–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
2. –ö–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
3. –ü–æ—á–µ–º—É —ç—Ç–æ –±—É–¥–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≤–æ Flourish

–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:
- **–¢–∏–ø**: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–∞—Ä—Ç–∞
  **–ö–æ–ª–æ–Ω–∫–∏**: –†–µ–≥–∏–æ–Ω, {target}
  **–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ**: –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
  **–ù–∞—Å—Ç—Ä–æ–π–∫–∏**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Ä–µ–≥–∏–æ–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ GeoJSON
"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∏."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ OpenAI API: {e}"

def cluster_data(df, n_clusters):
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    
    if not numeric_cols:
        return df, "–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[numeric_cols])
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)
    
    df['Cluster'] = clusters
    cluster_analysis = df.groupby('Cluster')[numeric_cols].mean().reset_index()
    
    return df, cluster_analysis

def show_results_tab():
    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"]:
        st.write("### –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")
        for metric, value in st.session_state['metrics'].items():
            st.metric(label=metric, value=f"{value:.4f}")
        
        if st.session_state['problem_type'] == "classification" and st.session_state['cm'] is not None:
            st.write("### –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
            try:
                # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                all_classes = np.unique(np.concatenate([
                    st.session_state['y_test'], 
                    st.session_state['y_pred']
                ]))
                
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
                fig, ax = plt.subplots(figsize=(8, 6))
                ConfusionMatrixDisplay.from_predictions(
                    st.session_state['y_test'],
                    st.session_state['y_pred'],
                    display_labels=all_classes,
                    cmap='Blues',
                    ax=ax,
                    values_format='d'
                )
                ax.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
                st.pyplot(fig)
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫: {str(e)}")
                
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
                try:
                    cm = confusion_matrix(st.session_state['y_test'], st.session_state['y_pred'])
                    unique_classes = np.unique(np.concatenate([
                        st.session_state['y_test'], 
                        st.session_state['y_pred']
                    ]))
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã –∏ –º–µ—Ç–æ–∫
                    if cm.shape[0] == len(unique_classes) and cm.shape[1] == len(unique_classes):
                        st.write(pd.DataFrame(
                            cm,
                            index=[f"–ò—Å—Ç–∏–Ω–Ω—ã–π {c}" for c in unique_classes],
                            columns=[f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π {c}" for c in unique_classes]
                        ))
                    else:
                        st.write("–ß–∏—Å–ª–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:", cm)
                except Exception as e2:
                    st.write("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫:", str(e2))
    
    elif ml_task == "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è":
        st.write("### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
        cluster_counts = st.session_state['df_clustered']['Cluster'].value_counts().sort_index()
        st.bar_chart(cluster_counts)
        
        st.write("### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        st.dataframe(st.session_state['cluster_analysis'])
        
        if len(df_clean.select_dtypes(include=np.number).columns) >= 2:
            num_cols = df_clean.select_dtypes(include=np.number).columns.tolist()
            col1, col2 = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –æ—Å—å X", num_cols, index=0), st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –æ—Å—å Y", num_cols, index=1)
            
            fig = px.scatter(
                st.session_state['df_clustered'],
                x=col1,
                y=col2,
                color='Cluster',
                hover_data=df_clean.columns.tolist(),
                title="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"
            )
            st.plotly_chart(fig, use_container_width=True)

def show_visualizations_tab():
    st.subheader("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"]:
        try:
            st.write("### –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (SHAP)")
            with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É—é SHAP-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é..."):
                fig = generate_shap_plot(
                    st.session_state['model'],
                    st.session_state['X_test'],
                    st.session_state['feature_names']
                )
                st.pyplot(fig)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ SHAP-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
        
        if st.session_state['problem_type'] == "regression":
            st.write("### –ü—Ä–æ–≥–Ω–æ–∑—ã vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            results = pd.DataFrame({
                '–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ': st.session_state['y_test'],
                '–ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ': st.session_state['y_pred']
            })
            
            try:
                fig = px.scatter(
                    results, 
                    x='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ', 
                    y='–ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ',
                    trendline='ols',
                    title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π"
                )
                st.plotly_chart(fig, use_container_width=True)
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç—Ä–µ–Ω–¥–ª–∏–Ω–∏—é: {str(e)}. –ü–æ–∫–∞–∑—ã–≤–∞—é scatter plot –±–µ–∑ –ª–∏–Ω–∏–∏ —Ç—Ä–µ–Ω–¥–∞.")
                fig = px.scatter(
                    results, 
                    x='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ', 
                    y='–ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ',
                    title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–±–µ–∑ —Ç—Ä–µ–Ω–¥–∞)"
                )
                st.plotly_chart(fig, use_container_width=True)

def show_report_tab():
    st.subheader("–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç—Å–∫–∏–π –æ—Ç—á–µ—Ç")
    
    if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"]:
        report = generate_ai_report(
            st.session_state['df'],
            st.session_state['model'],
            st.session_state['problem_type'],
            st.session_state['target'],
            st.session_state['metrics']
        )
        st.markdown(report)
        
        st.divider()
        
        st.write("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º (Flourish)")
        flourish_recs = generate_flourish_recommendations(
            st.session_state['df'],
            st.session_state['target']
        )
        if flourish_recs:
            st.markdown(flourish_recs)
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è Flourish")
    
    elif ml_task == "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è":
        st.write("### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        cluster_summary = st.session_state['cluster_analysis'].to_dict()
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –¥–ª—è –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∞:

–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:
{cluster_summary}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π:
1. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
2. –ö–∞–∫ –º–æ–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
3. –ò–¥–µ–∏ –¥–ª—è —Å—Ç–∞—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""
        try:
            if openai.api_key:
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "–¢—ã –∂—É—Ä–Ω–∞–ª–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1500
                )
                st.markdown(response['choices'][0]['message']['content'])
            else:
                st.warning("–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è OpenAI API –∫–ª—é—á")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ OpenAI API: {e}")

def show_settings_tab():
    st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")
    
    if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"]:
        # –°–æ–∑–¥–∞–µ–º –±—É—Ñ–µ—Ä –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        buffer = io.BytesIO()
        joblib.dump(st.session_state['model'], buffer)
        buffer.seek(0)
        
        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å (joblib)",
            data=buffer,
            file_name=f"model_{datetime.now().strftime('%Y%m%d')}.joblib",
            mime="application/octet-stream"
        )
        
        st.write("### –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑")
        sample = df_clean.drop(columns=[st.session_state['target']]).iloc[0:1]
        st.write("–î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:")
        st.dataframe(sample)
        
        if st.button("–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):
            sample_prepared = prepare_data_for_ml(sample, st.session_state['target'])[0]
            prediction = st.session_state['model'].predict(sample_prepared)
            st.metric(label="–ü—Ä–æ–≥–Ω–æ–∑", value=prediction[0])

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    st.sidebar.header("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ")
    uploaded_file = st.sidebar.file_uploader("CSV, Excel –∏–ª–∏ JSON", type=["csv", "xlsx", "xls", "json"])

    global df, df_clean, ml_task
    df = None
    df_clean = None

    if uploaded_file:
        df, error = load_data(uploaded_file)
        if df is not None:
            df = reduce_mem_usage(df)
            st.sidebar.success(f"–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {uploaded_file.name}")
            
            with st.expander("üîç –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö", expanded=True):
                st.dataframe(df.head(3))
                st.caption(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
            
            with st.spinner("üßπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—á–∏—â–∞—é –¥–∞–Ω–Ω—ã–µ..."):
                df_clean = fill_missing_values(df)
                df_clean = mark_anomalies(df_clean)
            
            st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã! –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'anomaly' –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π")
            
            col1, col2 = st.columns(2)
            with col1:
                csv = df_clean.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å CSV –¥–ª—è Flourish",
                    data=csv,
                    file_name=f"cleaned_data_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv",
                    help="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Flourish"
                )
            with col2:
                json_data = df_clean.to_json(orient='records', force_ascii=False)
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å JSON –¥–ª—è Flourish",
                    data=json_data,
                    file_name=f"cleaned_data_{datetime.now().strftime('%Y%m%d')}.json",
                    mime="application/json",
                    help="–§–æ—Ä–º–∞—Ç JSON –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"
                )
            
            st.sidebar.header("2. –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–¥–∞—á—É ML")
            ml_task = st.sidebar.selectbox("–¢–∏–ø –∑–∞–¥–∞—á–∏", 
                                         ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", 
                                          "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", 
                                          "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"],
                                         index=0)
            
            st.sidebar.header("3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
            
            if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"]:
                target_col = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é", df_clean.columns)
                
                if st.sidebar.button("‚ñ∂ –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
                    with st.spinner("üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                        problem_type = "regression" if ml_task == "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)" else "classification"
                        
                        X, y, scaler = prepare_data_for_ml(df_clean, target_col)
                        model, metrics, X_test, y_test, y_pred, cm = train_model(X, y, problem_type)
                        
                        st.session_state['model'] = model
                        st.session_state['metrics'] = metrics
                        st.session_state['X_test'] = X_test
                        st.session_state['y_test'] = y_test
                        st.session_state['y_pred'] = y_pred
                        st.session_state['cm'] = cm
                        st.session_state['feature_names'] = df_clean.drop(columns=[target_col]).columns.tolist()
                        st.session_state['target'] = target_col
                        st.session_state['problem_type'] = problem_type
                        st.session_state['df'] = df_clean
                        
                        st.success("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
            
            elif ml_task == "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è":
                n_clusters = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 10, 4)
                
                if st.sidebar.button("‚ñ∂ –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é", type="primary"):
                    with st.spinner("üîç –í—ã–ø–æ–ª–Ω—è—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é..."):
                        df_clustered, cluster_analysis = cluster_data(df_clean, n_clusters)
                        
                        st.session_state['df_clustered'] = df_clustered
                        st.session_state['cluster_analysis'] = cluster_analysis
                        
                        st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤!")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–æ –≤–∫–ª–∞–¥–∫–∞—Ö
            show_tabs = False
            if ml_task in ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"] and 'model' in st.session_state:
                show_tabs = True
            elif ml_task == "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è" and 'df_clustered' in st.session_state:
                show_tabs = True
            
            if show_tabs:
                tab1, tab2, tab3, tab4 = st.tabs(["üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", "üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "üìù –ñ—É—Ä–Ω–∞–ª–∏—Å—Ç—Å–∫–∏–π –æ—Ç—á–µ—Ç", "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏"])
                
                with tab1:
                    show_results_tab()
                
                with tab2:
                    show_visualizations_tab()
                
                with tab3:
                    show_report_tab()
                
                with tab4:
                    show_settings_tab()
            else:
                st.info("‚ÑπÔ∏è –ù–∞–∂–º–∏—Ç–µ '–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å' –∏–ª–∏ '–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é' —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        else:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {error}")
    else:
        st.info("üëà –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞")
        st.image("https://images.unsplash.com/photo-1551288049-bebda4e38f71?ixlib=rb-1.2.1&auto=format&fit=crop&w=1200&q=80", 
                 caption="–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—Å–∫–∏—Ö —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö")

if __name__ == "__main__":
    main()
