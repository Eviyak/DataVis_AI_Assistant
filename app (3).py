from sklearn.preprocessing import StandardScaler
from tabulate import tabulate
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import openai
import io
import json
import warnings
from plotly.subplots import make_subplots
from sklearn.ensemble import IsolationForest
from statsmodels.tsa.seasonal import seasonal_decompose
from pandas.api.types import is_datetime64_any_dtype

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="InsightBot Pro",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ API –∫–ª—é—á–∞ OpenAI –∏–∑ Streamlit Secrets
if 'OPENAI_API_KEY' in st.secrets:
    openai.api_key = st.secrets['OPENAI_API_KEY']
else:
    openai.api_key = ""

# –¢–µ–º–∞ –¥–Ω–µ–≤–Ω–∞—è (–±–µ–∑ –≤—ã–±–æ—Ä–∞)
st.markdown("""
    <style>
        .stApp { background-color: #f0f2f6; color: #000000; }
        footer { visibility: hidden; }
    </style>
""", unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("InsightBot Pro")
st.markdown("""
    <div style="background-color:#ffffff;padding:10px;border-radius:10px;margin-bottom:20px;">
    <p style="color:#333;font-size:18px;">üöÄ <b>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å AI-powered –∏–Ω—Å–∞–π—Ç–∞–º–∏</b></p>
    <p style="color:#666;">–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV, Excel –∏–ª–∏ JSON ‚Äî –ø–æ–ª—É—á–∏—Ç–µ –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é</p>
    </div>
""", unsafe_allow_html=True)

# === –ö–≠–® –ó–ê–ì–†–£–ó–ö–ò ===
@st.cache_data(show_spinner="–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ... ‚è≥", ttl=3600, max_entries=3)
def load_data(uploaded_file):
    try:
        file_bytes = uploaded_file.read()
        if uploaded_file.name.endswith('.csv'):
            return pd.read_csv(io.BytesIO(file_bytes), encoding_errors='ignore')
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            return pd.read_excel(io.BytesIO(file_bytes))
        elif uploaded_file.name.endswith('.json'):
            data = json.loads(file_bytes.decode('utf-8'))
            return pd.json_normalize(data)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
        return None

def reduce_mem_usage(df):
    start_mem = df.memory_usage().sum() / 1024**2
    for col in df.columns:
        col_type = df[col].dtype
        if col_type != object:
            try:
                c_min = df[col].min()
                c_max = df[col].max()
                if pd.isnull(c_min) or pd.isnull(c_max):
                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã

                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    else:
                        df[col] = df[col].astype(np.int64)
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
                    else:
                        df[col] = df[col].astype(np.float64)
            except (ValueError, TypeError):
                continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü, –µ—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞
    end_mem = df.memory_usage().sum() / 1024**2
    st.sidebar.info(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏: {start_mem:.2f} MB ‚Üí {end_mem:.2f} MB (—Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ {100*(start_mem-end_mem)/start_mem:.1f}%)")
    return df


@st.cache_data(show_spinner="–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ... üîç", ttl=600)
def analyze_with_ai(df):
    try:
        analysis = ""
        analysis += f"- **–°—Ç—Ä–æ–∫–∏:** {df.shape[0]}\n"
        analysis += f"- **–ö–æ–ª–æ–Ω–∫–∏:** {df.shape[1]}\n"
        analysis += f"- **–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö:** {df.memory_usage().sum() / 1024**2:.2f} MB\n\n"

        num_cols = df.select_dtypes(include=np.number).columns
        if len(num_cols) > 0:
            analysis += "### üî¢ –ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n"
            stats = df[num_cols].describe().transpose()
            stats['skew'] = df[num_cols].skew()
            analysis += stats[['mean', 'std', 'min', '50%', 'max', 'skew']].to_markdown()

        cat_cols = df.select_dtypes(exclude=np.number).columns
        if len(cat_cols) > 0:
            analysis += "\n\n### üî§ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n"
            for col in cat_cols:
                analysis += f"- **{col}**: {df[col].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n"

        missing = df.isnull().sum()
        if missing.sum() > 0:
            analysis += "\n\n### ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n"
            missing_percent = missing[missing > 0] / len(df) * 100
            missing_df = pd.DataFrame({'–ö–æ–ª–æ–Ω–∫–∞': missing_percent.index,
                                      '–ü—Ä–æ–ø—É—Å–∫–∏': missing[missing > 0],
                                      '%': missing_percent.values.round(1)})
            analysis += missing_df.to_markdown(index=False)

        if len(num_cols) > 1:
            corr = df[num_cols].corr().abs().unstack().sort_values(ascending=False)
            strong_corr = corr[(corr > 0.7) & (corr < 1)].drop_duplicates()
            if len(strong_corr) > 0:
                analysis += "\n\n### üîó –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n"
                for pair, value in strong_corr.items():
                    analysis += f"- {pair[0]} –∏ {pair[1]}: {value:.2f}\n"

        return analysis
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"

@st.cache_data(show_spinner="–ì–µ–Ω–µ—Ä–∏—Ä—É—é AI –∏–Ω—Å–∞–π—Ç—ã... ü§ñ", ttl=600)
def generate_ai_insights(df):
    if not openai.api_key:
        return "üîë –ö–ª—é—á OpenAI API –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets."

    prompt = (
        f"–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø–æ –¥–∞–Ω–Ω—ã–º.\n"
        f"–î–∞–Ω–Ω—ã–µ: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫.\n"
        f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}.\n"
        f"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\n{df.head().to_dict()}\n\n"
        f"–î–∞–π –∫—Ä–∞—Ç–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω—ã–º."
    )

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=400
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI API: {str(e)}"

@st.cache_data(show_spinner="–ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º... üé®", ttl=600)
def generate_viz_recommendations(df):
    if not openai.api_key:
        return None

    prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–æ–ª–æ–Ω–∫–∏ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {list(df.columns)}.
–ü—Ä–µ–¥–ª–æ–∂–∏ 3 –ø—Ä–æ—Å—Ç—ã–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤. 
–ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–æ. –ù–∞–ø—Ä–∏–º–µ—Ä:

- –ü–æ—Å—Ç—Ä–æ–π –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ 'Age'
- –ü–æ—Å—Ç—Ä–æ–π scatter plot —Å 'Height' –ø–æ –æ—Å–∏ X –∏ 'Weight' –ø–æ –æ—Å–∏ Y
- –ü–æ—Å—Ç—Ä–æ–π box plot –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ 'Salary'

–ü–∏—à–∏ –≤ —Ç–∞–∫–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ, –±–µ–∑ JSON –∏ –ª–∏—à–Ω–∏—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
"""

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=300
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ OpenAI API: {e}"

# ... (–≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏–º–ø–æ—Ä—Ç—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

# === UI ===
st.sidebar.header("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏")
uploaded_file = st.sidebar.file_uploader("CSV, Excel –∏–ª–∏ JSON", type=["csv", "xlsx", "xls", "json"])

if uploaded_file:
    df = load_data(uploaded_file)
    if df is not None:
        df = reduce_mem_usage(df)
        st.success(f"–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {uploaded_file.name} ({df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫)")
        st.dataframe(df.head())

        st.subheader("üìä –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
        summary = analyze_with_ai(df)
        st.markdown(summary)

        # === –î–ï–¢–ï–ö–¶–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô (ML-–ö–û–ú–ü–û–ù–ï–ù–¢) ===
        if st.sidebar.checkbox("üîç –ù–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö", True):
            try:
                numeric_cols = df.select_dtypes(include=np.number).columns
                if len(numeric_cols) > 0:
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    X = df[numeric_cols].fillna(df[numeric_cols].median())
                    X_scaled = StandardScaler().fit_transform(X)

                    # –ú–æ–¥–µ–ª—å
                    clf = IsolationForest(contamination=0.05, random_state=42)
                    df['anomaly_score'] = clf.fit_predict(X_scaled)
                    anomalies = df[df['anomaly_score'] == -1]

                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    if not anomalies.empty:
                        st.subheader("üö® –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏")
                        st.write(f"–ù–∞–π–¥–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π (Isolation Forest):")
                        st.dataframe(anomalies.head())

                        # –ì—Ä–∞—Ñ–∏–∫ –∞–Ω–æ–º–∞–ª–∏–π
                        col = numeric_cols[0]
                        fig = px.scatter(
                            df, x=df.index, y=col, 
                            color=df['anomaly_score'].astype(str),
                            title=f"–ê–Ω–æ–º–∞–ª–∏–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ '{col}'",
                            color_discrete_map={"-1": "red", "1": "blue"}
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.success("–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")
                else:
                    st.warning("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {str(e)}")

        st.subheader("ü§ñ AI –ò–Ω—Å–∞–π—Ç—ã –ø–æ –¥–∞–Ω–Ω—ã–º")
        insights = generate_ai_insights(df)
        st.markdown(insights)

        st.subheader("üé® –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º")
        viz_recs = generate_viz_recommendations(df)
        if viz_recs:
            st.markdown(viz_recs)
        else:
            st.info("–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º.")
    else:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞.")
else:
    st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
